{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Bug Predictor - Machine Learning Model\n",
    "\n",
    "This notebook trains a machine learning model to predict bugs in source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../dataset/bug_dataset_50k.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['has_bug'].value_counts())\n",
    "print(f\"\\nBug percentage: {df['has_bug'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select numerical features to visualize\n",
    "numerical_features = [\n",
    "    'loc', 'cyclomatic_complexity', 'halstead_volume',\n",
    "    'num_functions', 'num_loops', 'num_conditionals',\n",
    "    'num_try_except', 'num_null_checks', 'nested_depth'\n",
    "]\n",
    "\n",
    "for idx, feature in enumerate(numerical_features[:9]):\n",
    "    ax = axes[idx]\n",
    "    sns.histplot(data=df, x=feature, hue='has_bug', ax=ax, bins=30, kde=True)\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.set_xlabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df.corr(numeric_only=True)\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            square=True, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop('has_bug', axis=1)\n",
    "y = df['has_bug']\n",
    "\n",
    "# Feature selection - drop highly correlated features\n",
    "correlation_threshold = 0.9\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "print(f\"Features to drop due to high correlation: {to_drop}\")\n",
    "X = X.drop(columns=to_drop)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nTraining bug percentage: {y_train.mean():.2%}\")\n",
    "print(f\"Test bug percentage: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Feature importance (using Random Forest for initial assessment)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(data=feature_importance.head(15), x='importance', y='feature')\n",
    "plt.title('Top 15 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    solver='lbfgs',\n",
    "    C=1.0\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Training accuracy: {model.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Bug', 'Bug']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Bug', 'Bug'], \n",
    "            yticklabels=['No Bug', 'Bug'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature coefficients analysis\n",
    "coefficients = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'coefficient': model.coef_[0]\n",
    "}).sort_values('coefficient', ascending=False)\n",
    "\n",
    "print(\"Top 10 Positive Coefficients (increase bug probability):\")\n",
    "print(coefficients.head(10))\n",
    "print(\"\\nTop 10 Negative Coefficients (decrease bug probability):\")\n",
    "print(coefficients.tail(10))\n",
    "\n",
    "# Visualize coefficients\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red' if c > 0 else 'blue' for c in coefficients['coefficient']]\n",
    "plt.barh(coefficients['feature'], coefficients['coefficient'], color=colors)\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability distribution analysis\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Probability distribution for each class\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=pd.DataFrame({\n",
    "    'probability': y_pred_proba[y_test == 0],\n",
    "    'class': 'No Bug'\n",
    "}), x='probability', bins=30, kde=True, label='No Bug')\n",
    "sns.histplot(data=pd.DataFrame({\n",
    "    'probability': y_pred_proba[y_test == 1],\n",
    "    'class': 'Bug'\n",
    "}), x='probability', bins=30, kde=True, label='Bug')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Probability Distribution by Class')\n",
    "plt.legend()\n",
    "\n",
    "# Decision boundary analysis\n",
    "plt.subplot(1, 2, 2)\n",
    "thresholds = np.linspace(0, 1, 100)\n",
    "accuracies = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred_thresh))\n",
    "\n",
    "plt.plot(thresholds, accuracies)\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Decision Threshold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and scaler\n",
    "model_data = {\n",
    "    'model': model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': X.columns.tolist(),\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'roc_auc': roc_auc,\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test)\n",
    "}\n",
    "\n",
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"Model saved successfully as 'model.pkl'\")\n",
    "print(f\"Model Accuracy: {model_data['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC Score: {model_data['roc_auc']:.4f}\")\n",
    "print(f\"Features used: {len(model_data['feature_names'])}\")\n",
    "print(f\"Training samples: {model_data['training_samples']}\")\n",
    "print(f\"Test samples: {model_data['test_samples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the saved model\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "loaded_model = loaded_data['model']\n",
    "loaded_scaler = loaded_data['scaler']\n",
    "\n",
    "# Test prediction on sample data\n",
    "sample_features = X_test_scaled[:5]\n",
    "predictions = loaded_model.predict(sample_features)\n",
    "probabilities = loaded_model.predict_proba(sample_features)[:, 1]\n",
    "\n",
    "print(\"Sample predictions:\")\n",
    "for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "    print(f\"Sample {i+1}: Predicted={'Bug' if pred == 1 else 'No Bug'} (Probability: {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Deployment Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test function for the model\n",
    "def predict_bug_probability(features_dict):\n",
    "    \"\"\"\n",
    "    Predict bug probability for given features\n",
    "    \n",
    "    Args:\n",
    "        features_dict: Dictionary of feature names and values\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with prediction results\n",
    "    \"\"\"\n",
    "    # Convert to array in correct order\n",
    "    features = [features_dict.get(name, 0) for name in loaded_data['feature_names']]\n",
    "    features_array = np.array(features).reshape(1, -1)\n",
    "    \n",
    "    # Scale features\n",
    "    features_scaled = loaded_scaler.transform(features_array)\n",
    "    \n",
    "    # Predict\n",
    "    probability = loaded_model.predict_proba(features_scaled)[0, 1]\n",
    "    prediction = loaded_model.predict(features_scaled)[0]\n",
    "    \n",
    "    return {\n",
    "        'has_bug': bool(prediction),\n",
    "        'probability': float(probability),\n",
    "        'severity': 'high' if probability > 0.7 else 'medium' if probability > 0.4 else 'low'\n",
    "    }\n",
    "\n",
    "# Test with sample features\n",
    "sample_features_dict = {feature: np.random.rand() * 10 for feature in loaded_data['feature_names']}\n",
    "result = predict_bug_probability(sample_features_dict)\n",
    "\n",
    "print(\"Sample prediction:\")\n",
    "print(f\"Features: {len(sample_features_dict)}\")\n",
    "print(f\"Has Bug: {result['has_bug']}\")\n",
    "print(f\"Probability: {result['probability']:.4f}\")\n",
    "print(f\"Severity: {result['severity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"AI BUG PREDICTOR - MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Dataset Size: {len(df)} samples\")\n",
    "print(f\"Bug Rate: {df['has_bug'].mean():.2%}\")\n",
    "print(f\"Features Used: {len(loaded_data['feature_names'])}\")\n",
    "print(f\"Model Type: Logistic Regression\")\n",
    "print(f\"Test Accuracy: {loaded_data['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC Score: {loaded_data['roc_auc']:.4f}\")\n",
    "print()\n",
    "print(\"Top 5 Important Features:\")\n",
    "for feature in coefficients.head(5)['feature']:\n",
    "    print(f\"  - {feature}\")\n",
    "print()\n",
    "print(\"Model Successfully Saved to: model.pkl\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}